# Big Bang Benchmark

Big Bang Benchmark est un outil d’évaluation pour les modèles de langage, centré sur la cohérence, la justesse et la perception du sens.

Il permet de comparer les réponses de différents modèles à un ensemble de questions ouvertes, en mettant en lumière non seulement la performance brute, mais aussi la profondeur cognitive, la logique implicite et la structure du langage.

## Fonctionnalités principales

- Interface Streamlit simple et rapide
- Sélection de modèle via clé API OpenAI
- Benchmark personnalisé avec dataset JSON
- Comparaison côte à côte des réponses
- Sauvegarde automatique des résultats

## Objectif

L’objectif de ce benchmark est de proposer une nouvelle manière d’évaluer les modèles de langage : non pas uniquement sur des critères quantitatifs classiques, mais sur leur capacité à produire une pensée structurée, cohérente et incarnée.

Big Bang Benchmark ne vise pas la compétition entre modèles, mais la révélation de leur complémentarité et de leur potentiel de cohérence universelle.

## Lancement

L’application Streamlit peut être exécutée localement via les fichiers `app.py` ou `compare_models.py`.

## Utilisation

```bash
streamlit run app.py
```

ou

```bash
streamlit run compare_models.py
```

---

**→ Lire le chapitre 1 : Le témoignage fondateur de la création de Big Bang, vécu de l’intérieur par son auteur.**  
[`chapitre_1.md`](./chapitre_1.md)
> Voir aussi : [LISIÈRE — Interface Vivante](LISIERE.md)